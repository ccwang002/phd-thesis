\chapter{Mutation Pipeline QC}
\label{chap:mut-pipeline-qc}


\section{Summary}
We present a systematic analysis of the effects of synchronizing a large-scale, deeply characterized, multi-omic dataset to the current human reference genome, using updated software, pipelines, and annotations.
For each of 5 molecular data platforms in The Cancer Genome Atlas (TCGA)---mRNA and miRNA expression, single nucleotide variants, DNA methylation and copy number alterations---comprehensive sample, gene, and probe-level studies were performed, towards quantifying the degree of similarity between the \enquote*{legacy} GRCh37 (hg19) TCGA data and its GRCh38 (hg38) version as \enquote*{harmonized} by the Genomic Data Commons.
We offer gene lists to elucidate differences that remained after controlling for confounders, and strategies to mitigate their impact on biological interpretation.
Our results demonstrate that the hg19 and hg38 TCGA datasets are very highly concordant, promote informed use of either legacy or harmonized omics data, and provide a rubric that encourages similar comparisons as new data emerge and reference data evolve.


\section{Introduction}
Over the course of a decade The Cancer Genome Atlas (TCGA) helped usher in the era of extreme-scale team science, yielding numerous biological insights and many widely cited papers \cite{hutterc_zenklusenjc:CancerGenome2018}. Underlying this progress in understanding the molecular bases of cancer is one of the broadest, deepest, and most integratively characterized biological datasets ever assembled: on the order of 2 petabytes of primary and secondary data, in the form of 84,000 data aliquots from some 11,300 patients across 33 disease studies. Most TCGA samples were originally aligned against the Genome Reference Consortium build GRCh37 (hg19), with a small fraction (from the pilot phase of TCGA) having been aligned against NCBI Build 36.1 (hg18). Since TCGA was initiated, however, the research community has undergone tremendous evolution, not only in the characterization machinery, due to the enormous drop in sequencing costs, but also in the surrounding ecosystem of reference data, sequence alignment methods, variant calling tools, RNA quantification methods, quality controls used to help distinguish signal from noise, and analysis software. For this reason, the Genomic Data Commons (GDC, \url{https://gdc.cancer.gov/}) was conceived by the National Cancer Institute (NCI) as more than just a massive warehouse of digitized samples: instead, by harmonizing those samples to a uniform reference alignment and gene annotation, then characterizing samples with established tools in consistent workflows and providing updates at regular intervals, the GDC also helps navigate an orderly course through this sea of constant change. The GDC thus offers promise as a force-multiplier for researchers, who can now spend more time exploring their biological questions and less on resolving inconsistencies in data and software versions.

In this paper, we examine the results of the first major harmonization effort undertaken at the GDC: in which the corpus of legacy TCGA data was either aligned or lifted over to the GRCh38 build (hg38) with a GDC workflow assembled from updated versions of bioinformatic tools and reference files used by sequencing and characterization centers in TCGA. While the mechanics of evaluation varied for each data platform, owing largely to natural differences between them and/or how their hg19 counterparts were harmonized to hg38 (e.g., re-alignment of single nucleotide variants [SNVs] versus liftover of SNP6 copy number arrays), in each case \enquote{the aim was to categorize observed differences in analytic results as a function of their sources and control for such to discern potential impact upon biological interpretation.} The sources of variation are given in a figure for each platform and include, among others: (1) genome reference; (2) gene annotation: e.g. UCSC genes, GENCODE, miRBase; (3) upstream methods used in alignment, variant calling and quantification, including: BWA \cite{lih_durbinr:BWAShortRead2009}, STAR \cite{dobina_gingerastr:STARUltrafast2013}, RSEM \cite{lib_deweycn:RSEMAccurate2011}, FPKM-HTSeq \cite{anderss_huberw:HTSeqPython2015}, and MuTect \cite{cibulskisk_getzg:SensitiveDetection2013}; (4) downstream methods used in clustering, correlation, or significance analysis, such as GISTIC \cite{mermelch_getzg:GISTIC2Facilitates2011}; (5) parameterizations such as: thresholds for filtering, p-values or q-values; and (6) auxiliary data, such as: GISTIC marker files and CNV lists, or panels of normals used to remove suspect SNVs. In the interest of reproducibility, the supplement describes the software codes and parameterizations used to carry out these studies, and for each platform includes manifests of the input files upon which our analyses were executed. In the remainder of the text we use the terms ``legacy data'' and ``harmonized data'' interchangeably with ``hg19 data'' and ``hg38 data'', respectively.
